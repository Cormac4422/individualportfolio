<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>individualportfolio</title>
</head>
<body>
  <h1>Week 4, Social Media & the Web - Principles within "A Contract for the Web" / Responses to "The Internet's Own Boy"</h1>
  
  <p>Based on Eli Pariser's TED Talk and the principles of the Contract for the Web, the interaction between Principle 5, protecting privacy and personal data, and Principle 8, building strong and respectful communities, is foundational. One principle enables the other. The filter bubble illustrates a key tension. The algorithms that use personal data to curate content often undermine healthy public discourse. When platforms track and personalize feeds without transparency, they erode trust and fragment shared reality. Communities built on isolated information streams struggle to achieve genuine civil discourse, as members do not operate in the same informational space. This dynamic is not absolute. Legal, geographic, and technical factors diversify online experiences. Yet the pervasive data collection that fuels filter bubbles challenges both principles. It commodifies privacy and divides community spaces. Protecting privacy is about agency. When users feel their data is respected, they are more likely to engage authentically, fostering communities built on trust rather than surveillance. These principles coalesce around the idea of agency. Principle 5 ensures individuals have control over their digital selves, while Principle 8 ensures the collective space is worthy of that trust. Without privacy, communities become vulnerable to manipulation. Without dignified discourse, privacy becomes a mere technical feature. The challenge is to design platforms that prioritize these interdependent values. Real change requires both responsible platforms and conscious users who demand spaces built on these foundations.</p>

  <p>The movie shows us that knowledge isn't free for everyone. Big companies and colleges can lock it up and charge money, even for research that regular people paid for with their taxes. This is like turning the free internet into a bunch of expensive toll roads. Aaron Swartz tried to fight this by sharing locked-up articles. He got in very big trouble for it. His story teaches us an important lesson: information should be shared, not hidden. We need to make rules that keep knowledge open for everyone, not just for people who can pay. We should remember him and try to make the internet a fairer place.</p> 

  <h1>Response to - Should All Archives Be Open Access?</h1>

  <p>Reading Rev's article shows that open archives are not a simple choice. His three main points explain why. First, opening archives can hurt people. Old records often have private details or painful stories. Putting them online could cause harm or shame to those people or their families. Second, a document can mean different things in different places. What is safe in one country might be dangerous in another. Once something is online, it can be taken out of context or used in the wrong way. Third, it is hard to open everything. There are costs, copyright laws, and some places do not have good internet access. Not every archive can put its records online fairly. I think we should open archives carefully. It helps researchers and learners when information is shared. But we must always protect people's privacy and safety first.</p>

  <h1>What is Artificial Intelligence?</h1>
  <p>Joy Buolamwini’s TED Talk shows that Artificial Intelligence is not what many people think. It is a system built by humans to find patterns in data. It does not see or understand the world. It only sees the information we give it. Her talk proves AI is not actually intelligent. She showed how face-scanning software could not detect her face until she wore a white mask. The AI was trained mostly on light-skinned male faces so it failed with others. This shows AI does not think. It only repeats the patterns and even the mistakes from its data. Her talk made me worry about the coming AI revolution. This technology is already used in important places like police work and job applications. If the data has human bias the AI will copy that bias and make unfair decisions on a huge scale. This is dangerous. The revolution will not be managed by machines. It will be shaped by our choices. We must demand that the people who build AI use fair and complete data. We must make rules to keep it accountable. AI is a powerful tool but it is our responsibility to make sure it helps people and does not harm them.</p>

  <h1>Responses to Anatomy of an AI System</h1>
  <p>Reading the article changed how I see AI. I thought it was just computer code or a smart program in the cloud. The article shows it is a physical thing built from the earth with a heavy human and environmental cost. It starts with people mining metals and minerals from the ground. This work is often dangerous. These raw materials are shipped around the world to factories. Workers assemble them into phones and smart speakers. The finished devices are shipped again to stores and our homes. Then we become part of the system. When we talk to a voice assistant or use a search engine we are not just consumers. We are providing free labor. Our voices and questions are the data that trains the AI to be smarter. The article says we are users, workers, and products all at the same time. This idea is powerful. It makes me look at the technology in my house differently. My smart speaker is not just a helpful gadget. It is the endpoint of a long chain of work and resource use. The article was an important eye opener. It shows the full story of AI from the ground to our living rooms. To talk about AI ethics or the future we must first understand this real physical system and its true cost.</p>

  <h1>What potential applications for VR, AR and XR do you envisage in future?</h1>
  <p>I think VR, AR and XR will be used in many helpful ways in the future. In education these tools can make learning real. Students could use VR to visit ancient Rome or walk inside a human heart. AR could make a textbook come to life showing 3D models right on the page. In healthcare the benefits could be great. Surgeons could practice operations in VR many times with no risk. During real surgery AR could show a patient's scans floating in the air helping the surgeon see exactly where to cut. This could save lives. These tools could also change how we work and connect. Instead of a video call you could put on a headset and sit in a virtual room with coworkers from around the world. You could share and change 3D designs together as if they were real objects on a table. However there are risks. If these worlds become too good people might not want to leave them. We must use VR and AR to improve real life not escape from it. The goal should be to help us learn, heal and work better, not to replace the people and places around us.</p>

  <h2>References</h2>
  <p>World Wide Web Foundation. Contract for the Web. 2019.</p>
<p>Pariser, Eli. “Beware Online ‘Filter Bubbles.’” TED, 2011.</p>
<p>Knappenberger, Brian. The Internet's Own Boy: The Story of Aaron Swartz. 2014.</p>
<p>Buolamwini, Joy. “How I’m Fighting Bias in Algorithms.” TED, 2018.</p>
<p>Crawford, Kate, and Vladan Joler. Anatomy of an AI System. 2018.</p>
</body>
</html>


